# AllData 一站式大数据平台

## [原型](https://orgnext.modao.cc/app/HhitGZQTr954c7Ug8XBvAY) ｜ [官方文档](https://alldatacenter.github.io/) ｜ [Document](https://github.com/alldatacenter/alldata/blob/master/README.md) ｜ [Community](#community)


## Stargazers over time

[![Stargazers over time](https://starchart.cc/alldatacenter/alldata.svg)](https://starchart.cc/alldatacenter/alldata)


<a href="https://github.com/alldatadc/github-readme-stats">
  <img width="1215" align="center" src="https://github-readme-stats.vercel.app/api?username=alldatadc&hide=stars&show_icons=true" />
</a>
<br/>
<br/>
<a href="https://github.com/alldatacenter/github-readme-stats">
  <img width="1215" align="center" src="https://github-readme-stats.vercel.app/api/pin/?username=alldatacenter&repo=alldata" />
</a>

## [体验版地址](http://43.138.157.47/dashboard) ｜ 账密 poc/123456


## 体验版
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204965509-fc13050b-ebe8-4bd5-8882-69e1af0a8367.png">
<br/>

> 首页

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204965519-d8fc6e7d-235d-4b52-82f6-358b3863d724.png">
<br/>

> 数据集成

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071504-c0e2b3ca-e3c2-4d70-8213-55c7316465ff.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071513-61997f31-12c4-464b-897d-bb60f0095ee9.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071519-35651c39-38c9-4746-ba18-c110a672e48f.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071527-d2af118c-ea4e-49f7-8435-c2b9ab8039cc.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071530-408d3001-0596-440c-93da-60d50aaacebe.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071537-4c0c2b38-dbfa-46a1-850e-4b73d5ca4236.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071539-f6b95712-4a32-481c-b719-f56c7542888c.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071544-43c5136d-091d-4602-b6de-ef528f39095c.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071554-1bc698f3-41d4-4fc5-ac74-a419ffede30e.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206071565-1634a747-0f60-4dcd-a294-90d80d59cdc7.png">
<br/>


> 元数据管理

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203520997-9ac122bb-f61a-4ea9-becf-efa0b72320ad.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203521639-ed23fceb-96ef-49e2-ac49-931978d7dcc0.png">
<br/>

> 元数据拾取
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203521695-8f7469db-fc98-44e2-b82c-de729cd5c9e1.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203521746-5d43dcc1-7ed3-4e50-b3b1-8fa767c34b26.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203521777-1c0a2635-27a5-42f3-a985-2d68556ed9d0.png">
<br/>


> 应用分析
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203521541-3a3186fd-0827-4c4c-bca0-f2761d6f4d67.png">
<br/>

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203521584-4099710f-b31f-4815-9c7f-e08e0cdc45bf.png">
<br/>

> 系统菜单管理
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203521695-8f7469db-fc98-44e2-b82c-de729cd5c9e1.png">
<br/>

> 元数据管理
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204560882-bc7876d5-6c35-4b79-aa39-aa2803f46e8c.png">
<br/>

> 数据质量
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204560895-d03ec73a-f670-4238-860e-26b708c3f2b7.png">
<br/>


> 数据市场
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204560915-6c17f056-0956-4a83-94a6-00cd21571af9.png">
<br/>


> 数据标准
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204561067-831d0733-812a-4660-a198-8248eaaf2d8c.png">
<br/>

> BI报表
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204561081-30169347-8aff-4150-a8c2-f3f3a5a2a20d.png">
<br/>

> 数据资产
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204561280-3d656a33-9e3a-440e-ab37-93397fe26b04.png">
<br/>

> 流程编排
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/204561326-002e0e59-e89d-47b4-8648-ca932194937b.png">
<br/>

> AllData AI Studio 社区版
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/211255550-2d58eb94-42ce-411c-9487-9f2e499e565a.png">
<br/>

> AllData Studio 社区版
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/210299541-b9f4d344-30ba-4fc9-a083-390129f7da1e.png">
<br/>

### 数据中台建设方法论

> 确定数据中台的目标和范围: 在开始构建数据中台之前, 需要明确数据中台的目标和范围, 例如数据中台的主要功能, 应用场景, 覆盖范围等
>
> 收集数据源: 根据定义的目标和范围, 收集相关的数据源这些数据源可以包括内部和外部的结构化和非结构化数据, 实时数据, 历史数据等
>
> 数据预处理: 对收集到的数据进行预处理, 包括数据清洗, 归一化, 去重, 脱敏等操作, 以确保数据的准确性, 一致性和安全性
>
> 数据建模和集成: 对预处理的数据进行建模和集成, 包括定义数据模型, 设计数据流程, 数据映射和转换等操作, 以确保数据的结构和语义一致性
>
> 数据存储和管理: 将集成的数据存储在数据仓库或数据湖中, 并实现数据的管理, 备份和恢复
>
> 数据治理和质量管理: 实现数据质量监控, 数据血缘追溯, 数据安全保障, 合规性检查等数据治理和质量管理功能, 以保证数据的高质量和可信性
>
> 数据分析和应用: 基于数据中台, 实现数据分析和应用, 如数据挖掘, 数据可视化, 机器学习等, 以提高数据的价值和应用效果
>
> 持续优化和创新: 数据中台的建设是一个持续迭代的过程, 需要不断进行优化和创新, 以适应不断变化的数据需求和业务场景
>
> 需要注意的是, 数据中台建设需要依赖于先进的技术和方法, 如云计算, 大数据, 人工智能, 数据湖等
>
> 同时, 数据中台建设还需要依赖于跨部门的协同和配合, 以确保数据的一致性和可用性

### 功能一览

- 平台基础设置
    - 系统管理
        - 岗位管理: 配置系统用户所属担任职务
        - 部门管理: 配置系统组织机构, 树结构展现支持数据权限
        - 菜单管理: 配置系统菜单, 操作权限, 按钮权限标识等
        - 角色管理: 角色菜单权限分配, 设置角色按机构进行数据范围权限划分
        - 用户管理: 用户是系统操作者, 该功能主要完成系统用户配置
        - 参数管理: 对系统动态配置常用参数
        - 字典管理: 对系统中经常使用的一些较为固定的数据进行维护
    - 系统监控
        - 登录日志: 系统登录日志记录查询
        - 操作日志: 系统正常操作日志记录和查询, 系统异常信息日志记录和查询
    - 任务调度
        - 任务管理: 在线（添加, 修改, 删除)任务调度
        - 日志管理: 任务调度执行结果日志
- 元数据管理
    - 数据源: 数据源连接信息管理, 可生成数据库文档
    - 元数据: 数据库表的元数据信息管理
    - 数据授权: 设置元数据信息权限划分
    - 变更记录: 元数据信息变更记录信息管理
    - 数据检索: 数据源, 数据表, 元数据等信息查询
    - 数据地图: 元数据的隶属数据表, 数据库的图形展示
    - SQL工作台: 在线执行查询sql
- 数据标准管理
    - 标准字典: 国标数据维护
    - 对照表: 本地数据中需要对照标准的数据维护
    - 字典对照: 本地数据与国标数据的对照关系
    - 对照统计: 本地数据与国标数据的对照结果统计分析
- 数据质量管理
    - 规则配置: 数据质量规则配置
    - 问题统计: 数据质量规则统计
    - 质量报告: 数据质量结果统计分析
    - 定时任务: 数据质量定时任务
    - 任务日志: 数据质量定时任务日志
- 主数据管理
    - 数据模型: 主数据数据模型维护
    - 数据管理: 主数据数据管理
- 数据集市管理
    - 数据服务: 动态开发api数据服务, 可生成数据服务文档
    - 数据脱敏: api数据服务返回结果动态脱敏
    - 接口日志: api数据服务调用日志
    - 服务集成: 三方数据服务集成管理
    - 服务日志: 三方数据服务集成调用日志
- 可视化管理
    - 数据集: 基于sql的查询结果维护
    - 图表配置: 动态echarts图表配置, 支持多维表格, 折线, 柱状, 饼图, 雷达, 散点等多种图表
    - 看板配置: 拖拽式添加图表组件, 调整位置, 大小
    - 酷屏配置: 拖拽式添加图表组件, 调整背景图, 颜色, 位置, 大小
- 流程管理
    - 流程定义: 流程定义管理
    - 流程实例
        - 运行中的流程: 运行中的流程实例管理
        - 我发起的流程: 我发起的流程实例管理
        - 我参与的流程: 我参与的流程实例管理
    - 流程任务
        - 待办任务: 待办任务管理
        - 已办任务: 已办任务管理
    - 业务配置: 配置业务系统与流程的相关属性


### 项目结构
项目采用按功能分模块的开发方式, 结构如下

- `common` 为系统的公共模块, 各种工具类, 公共配置存在该模块

- `system` 为系统核心模块也是项目入口模块, 也是最终需要打包部署的模块

- `logging` 为系统的日志模块, 其他模块如果需要记录日志需要引入该模块

- `tools` 为第三方工具模块, 包含: 图床, 邮件, 云存储, 本地存储, 支付宝

- `generator` 为系统的代码生成模块, 代码生成的模板在 system 模块中

### 详细结构

```
- common 公共模块
    - annotation 为系统自定义注解
    - aspect 自定义注解的切面
    - base 提供了Entity, DTO基类和mapstruct的通用mapper
    - config 自定义权限实现, redis配置, swagger配置, Rsa配置等
    - exception 项目统一异常的处理
    - utils 系统通用工具类
- system 系统核心模块（系统启动入口）
	- config 配置跨域与静态资源, 与数据权限
	    - thread 线程池相关
	- modules 系统相关模块(登录授权, 系统监控, 定时任务, 运维管理等)
- logging 系统日志模块
- tools 系统第三方工具模块
- generator 系统代码生成模块
```

### 主要技术栈

### 后端技术栈

- 开发框架: Spring Boot 2.3
- 微服务框架: Spring Cloud Hoxton.SR9
- 安全框架: Spring Security + Spring OAuth 2.0
- 任务调度: Quartz
- 持久层框架: MyBatis Plus
- 数据库连接池: Hikaricp
- 服务注册与发现: Spring Cloud Config
- 客户端负载均衡: Ribbon
- 熔断组件: Hystrix
- 网关组件: Spring Cloud Gateway
- 消息队列: Rabbitmq
- 缓存: Redis
- 日志管理: Logback
- 运行容器: Undertow
- 工作流: Flowable 6.5.0

### 前端技术栈

- JS框架: Vue, nodejs
- CSS框架: sass
- 组件库: ElementUI
- 打包构建工具: Webpack



### 部署方式

> 数据库版本为 **mysql5.7** 及以上版本
>
#### 1, `studio`数据库初始化
>
> 1.1 source install/16gmaster/studio/studio_alldatadc.sql
>
> 1.2 source install/16gmaster/studio/studio_dts.sql
>
> 1.3 source install/16gmaster/studio/studio_data_cloud.sql
>
> 1.4 source install/16gmaster/studio/studio_cloud_quartz.sql
>
> 1.5 source install/16gmaster/studio/studio_foodmart2.sql
>
> 1.6 source install/16gmaster/studio/studio_robot.sql
>
#### 2, 修改 **config** 配置中心

> **config** 文件夹下的配置文件, 修改 **redis**, **mysql** 和 **rabbitmq** 的配置信息
>
#### 3, 安装aspose-words

> cd install/16gmaster/studio
>
> mvn install:install-file -DgroupId=com.aspose -DartifactId=aspose-words -Dversion=20.3 -Dpackaging=jar -Dfile=aspose-words-20.3.jar
>
#### 4, 项目根目录下执行 **mvn install**
>
> 获取安装包build/studio-release-0.3.2.tar.gz
>
> 上传服务器解压
>
#### 5, 部署微服务: 进入不同的目录启动相关服务
>
> 5.1 必须启动, 并且顺序启动
>
> eureka->config->gateway
>
> 5.2 按需启动`cd install/16gmaster`
>
> 譬如启动元数据管理
>
> sh `install/16gmaster/data-metadata-service.sh`
>
> tail -100f `install/16gmaster/data-metadata-service.log`
>
> 5.2 按需启动`cd install/16gdata`
>
> 按需启动相关服务
>
> 5.3 按需启动`cd install/16gslave`
>
> 按需启动相关服务
>
>

#### 6, 部署`studio`:
>
> 6.1 启动`sh install/16gmaster/system.sh`
>
> 6.2 部署`studio`前端
>
> source /etc/profile
>
> cd $(dirname $0)
>
> source /root/.bashrc && nvm use v10.15.3
>
> nohup npm run dev &
>
> 6.3 访问`studio`页面
>
> curl http://localhost:8013
>
> 用户名: admin 密码: 123456

## DataHub本地开发、构建、启动 On Linux
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221186552-50bf2644-3ce6-4a22-944d-bfd7ab0d91f0.png">
<br/>
### DataHub源码构建
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221185131-5420b956-4ffb-4041-a286-166f02907954.png">
<br/>
### 命令行安装成功
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221185319-3f0046bf-5ed3-4602-a615-3ca638bbcf0a.png">
<br/>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221185417-4a095557-80ef-4c37-b32c-203c4b06c53f.png">
<br/>

### DataHub架构
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221185460-caa019b9-04f3-4b64-9550-b6d192075a37.png">
<br/>

> DataHub (& GMA) 架构
> DatahHub 采用前后端分离 + 微服务 / 容器架构
> 前端：Ember + TypeScript + ES9 + ES.Next + Yarn + ESLint
> 
> 服务端：Play Framework（web 框架） + Spring + Rest.li（restful 框架）+ Pegasus（数据建模语言） + Apache Samza （流处理框架）
> 
> 基础设施：elastic search (5.6) + Mysql + neo4j + kafka
> 
> 构建工具：Gradlew + Docker + Docker compose
> 
> DataHub 组成
> 
> datahub-gms (Generalized Metadata Store) ： 元数据存储服务
> 
> datahub-gma (Generalized Metadata Architecture) ： 通用元数据体系结构
> 
> GMA 是 datahub 的基础设施，提供标准化的元数据模型和访问层
> 
> datahub-frontend ： 应用前端
> 
> datahub-mxe 元数据事件datahub-mce-consumer （MetadataChangeEvent）：元数据变更事件，由平台或爬虫程序发起，写入到 GMS
> 
> datahub-mae-consumer (MetadataAuditEvent)： 元数据审计事件，只有被成功处理的 MCE 才会产生相应的 MAE，由 GMS 发起 ，写入到 es&Neo4j

### 1、JAVA_HOME
> 1.1 安装Java-11 && 配置JAVA_HOME
>
> sudo yum install java-11-openjdk -y
> 
> sudo yum install java-11-openjdk-devel
>
> 1.2 安装Java-8 && 不需要配置JAVA_HOME
>
> yum install java-1.8.0-openjdk.x86_64
>
> yum install -y java-1.8.0-openjdk-devel.x86_64

### 2、Python3.7以上版本
> 2.1 下载python3.7 
> 
> mkdir -p /usr/local/python3 && cd /usr/local/python3
> 
> wget https://www.python.org/ftp/python/3.7.16/Python-3.7.16.tar.xz
> 
> tar -xvf Python-3.7.16.tar.xz
> 
> cd Python-3.7.16
> 
> ./configure --prefix=/usr/local/python3
>
> make && make install
> 
> ln -s /usr/local/python3/bin/python3 /usr/bin/python3
> 
> 验证python3.7版本

### 3、源码构建
> 3.1 安装sasl、fastjsonschema
>
> 3.1.1 yum -y install cyrus-sasl cyrus-sasl-devel cyrus-sasl-lib
> 
> 3.1.2 pip3 install fastjsonschema
> 
> 3.1.3 yum -y install openldap-devel
> 
> 3.1.4 pip3 install python_ldap
> 
> 3.1.5 cd cd smoke-test && pip install -r requirements.txt
> 
> 3.2 安装命令行
> 
> ./gradlew :metadata-ingestion:installDev
> 
> 3.3 后端打包 
> 
> 执行./gradlew metadata-service:war:build
> 
> 3.4 前端打包 
> 
> 修改node版本: 找到datahub-0.10.0/datahub-web-react/build.gradle, 修改version为'16.10.0'
>
> export NODE_OPTIONS="--max-old-space-size=8192"
> 
> 执行./gradlew :datahub-frontend:dist -x yarnTest -x yarnLint

### 4 启动datahub
> 新增docker-compose.yml
```
networks:
  default:
    name: datahub_network
services:
  broker:
    container_name: broker
    depends_on:
      - zookeeper
    environment:
      - KAFKA_BROKER_ID=1
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
      - KAFKA_LISTENER_SECURITY_PROTOCOL_MAP=PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      - KAFKA_ADVERTISED_LISTENERS=PLAINTEXT://broker:29092,PLAINTEXT_HOST://localhost:9092
      - KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=1
      - KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=0
      - KAFKA_HEAP_OPTS=-Xms256m -Xmx256m
      - KAFKA_CONFLUENT_SUPPORT_METRICS_ENABLE=false
    hostname: broker
    image: confluentinc/cp-kafka:7.2.2
    ports:
      - ${DATAHUB_MAPPED_KAFKA_BROKER_PORT:-9092}:9092
  datahub-actions:
    depends_on:
      - datahub-gms
    environment:
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_GMS_PROTOCOL=http
      - DATAHUB_SYSTEM_CLIENT_ID=__datahub_system
      - DATAHUB_SYSTEM_CLIENT_SECRET=JohnSnowKnowsNothing
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - KAFKA_PROPERTIES_SECURITY_PROTOCOL=PLAINTEXT
      - METADATA_AUDIT_EVENT_NAME=MetadataAuditEvent_v4
      - METADATA_CHANGE_LOG_VERSIONED_TOPIC_NAME=MetadataChangeLog_Versioned_v1
      - SCHEMA_REGISTRY_URL=http://schema-registry:8081
    hostname: actions
    image: acryldata/datahub-actions:${ACTIONS_VERSION:-head}
    restart: on-failure:5
  datahub-frontend-react:
    container_name: datahub-frontend-react
    depends_on:
      - datahub-gms
    environment:
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - DATAHUB_SECRET=YouKnowNothing
      - DATAHUB_APP_VERSION=1.0
      - DATAHUB_PLAY_MEM_BUFFER_SIZE=10MB
      - JAVA_OPTS=-Xms512m -Xmx512m -Dhttp.port=9002 -Dconfig.file=datahub-frontend/conf/application.conf -Djava.security.auth.login.config=datahub-frontend/conf/jaas.conf -Dlogback.configurationFile=datahub-frontend/conf/logback.xml -Dlogback.debug=false -Dpidfile.path=/dev/null
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - DATAHUB_TRACKING_TOPIC=DataHubUsageEvent_v1
      - ELASTIC_CLIENT_HOST=elasticsearch
      - ELASTIC_CLIENT_PORT=9200
    hostname: datahub-frontend-react
    image: ${DATAHUB_FRONTEND_IMAGE:-linkedin/datahub-frontend-react}:${DATAHUB_VERSION:-head}
    ports:
      - ${DATAHUB_MAPPED_FRONTEND_PORT:-9002}:9002
    volumes:
      - ${HOME}/.datahub/plugins:/etc/datahub/plugins
  datahub-gms:
    container_name: datahub-gms
    depends_on:
      - mysql
    environment:
      - DATAHUB_SERVER_TYPE=${DATAHUB_SERVER_TYPE:-quickstart}
      - DATAHUB_TELEMETRY_ENABLED=${DATAHUB_TELEMETRY_ENABLED:-true}
      - DATAHUB_UPGRADE_HISTORY_KAFKA_CONSUMER_GROUP_ID=generic-duhe-consumer-job-client-gms
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - EBEAN_DATASOURCE_HOST=mysql:3306
      - EBEAN_DATASOURCE_PASSWORD=datahub
      - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
      - EBEAN_DATASOURCE_USERNAME=datahub
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
      - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
      - ELASTICSEARCH_PORT=9200
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
      - ENTITY_SERVICE_ENABLE_RETENTION=true
      - ES_BULK_REFRESH_POLICY=WAIT_UNTIL
      - GRAPH_SERVICE_DIFF_MODE_ENABLED=true
      - GRAPH_SERVICE_IMPL=elasticsearch
      - JAVA_OPTS=-Xms1g -Xmx1g
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - KAFKA_SCHEMAREGISTRY_URL=http://schema-registry:8081
      - MAE_CONSUMER_ENABLED=true
      - MCE_CONSUMER_ENABLED=true
      - PE_CONSUMER_ENABLED=true
      - UI_INGESTION_ENABLED=true
    hostname: datahub-gms
    image: ${DATAHUB_GMS_IMAGE:-linkedin/datahub-gms}:${DATAHUB_VERSION:-head}
    ports:
      - ${DATAHUB_MAPPED_GMS_PORT:-8080}:8080
    volumes:
      - ${HOME}/.datahub/plugins:/etc/datahub/plugins
  datahub-upgrade:
    command:
      - -u
      - SystemUpdate
    container_name: datahub-upgrade
    environment:
      - EBEAN_DATASOURCE_USERNAME=datahub
      - EBEAN_DATASOURCE_PASSWORD=datahub
      - EBEAN_DATASOURCE_HOST=mysql:3306
      - EBEAN_DATASOURCE_URL=jdbc:mysql://mysql:3306/datahub?verifyServerCertificate=false&useSSL=true&useUnicode=yes&characterEncoding=UTF-8
      - EBEAN_DATASOURCE_DRIVER=com.mysql.jdbc.Driver
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - KAFKA_SCHEMAREGISTRY_URL=http://schema-registry:8081
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_INDEX_BUILDER_MAPPINGS_REINDEX=true
      - ELASTICSEARCH_INDEX_BUILDER_SETTINGS_REINDEX=true
      - ELASTICSEARCH_BUILD_INDICES_CLONE_INDICES=false
      - GRAPH_SERVICE_IMPL=elasticsearch
      - DATAHUB_GMS_HOST=datahub-gms
      - DATAHUB_GMS_PORT=8080
      - ENTITY_REGISTRY_CONFIG_PATH=/datahub/datahub-gms/resources/entity-registry.yml
    hostname: datahub-upgrade
    image: ${DATAHUB_UPGRADE_IMAGE:-acryldata/datahub-upgrade}:${DATAHUB_VERSION:-head}
  elasticsearch:
    container_name: elasticsearch
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - ES_JAVA_OPTS=-Xms256m -Xmx512m -Dlog4j2.formatMsgNoLookups=true
    healthcheck:
      retries: 4
      start_period: 2m
      test:
        - CMD-SHELL
        - curl -sS --fail 'http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=0s' || exit 1
    hostname: elasticsearch
    image: elasticsearch:7.10.1
    mem_limit: 1g
    ports:
      - ${DATAHUB_MAPPED_ELASTIC_PORT:-9200}:9200
    volumes:
      - esdata:/usr/share/elasticsearch/data
  elasticsearch-setup:
    container_name: elasticsearch-setup
    depends_on:
      - elasticsearch
    environment:
      - ELASTICSEARCH_HOST=elasticsearch
      - ELASTICSEARCH_PORT=9200
      - ELASTICSEARCH_PROTOCOL=http
    hostname: elasticsearch-setup
    image: ${DATAHUB_ELASTIC_SETUP_IMAGE:-linkedin/datahub-elasticsearch-setup}:${DATAHUB_VERSION:-head}
  kafka-setup:
    container_name: kafka-setup
    depends_on:
      - broker
      - schema-registry
    environment:
      - DATAHUB_PRECREATE_TOPICS=${DATAHUB_PRECREATE_TOPICS:-false}
      - KAFKA_BOOTSTRAP_SERVER=broker:29092
      - KAFKA_ZOOKEEPER_CONNECT=zookeeper:2181
    hostname: kafka-setup
    image: ${DATAHUB_KAFKA_SETUP_IMAGE:-linkedin/datahub-kafka-setup}:${DATAHUB_VERSION:-head}
  mysql:
    command: --character-set-server=utf8mb4 --collation-server=utf8mb4_bin --default-authentication-plugin=mysql_native_password
    container_name: mysql
    environment:
      - MYSQL_DATABASE=datahub
      - MYSQL_USER=datahub
      - MYSQL_PASSWORD=datahub
      - MYSQL_ROOT_PASSWORD=datahub
    hostname: mysql
    image: mysql:5.7
    ports:
      - ${DATAHUB_MAPPED_MYSQL_PORT:-33061}:3306
    volumes:
      - ../mysql/init.sql:/docker-entrypoint-initdb.d/init.sql
      - mysqldata:/var/lib/mysql
  mysql-setup:
    container_name: mysql-setup
    depends_on:
      - mysql
    environment:
      - MYSQL_HOST=mysql
      - MYSQL_PORT=3306
      - MYSQL_USERNAME=datahub
      - MYSQL_PASSWORD=datahub
      - DATAHUB_DB_NAME=datahub
    hostname: mysql-setup
    image: ${DATAHUB_MYSQL_SETUP_IMAGE:-acryldata/datahub-mysql-setup}:${DATAHUB_VERSION:-head}
  schema-registry:
    container_name: schema-registry
    depends_on:
      - broker
    environment:
      - SCHEMA_REGISTRY_HOST_NAME=schemaregistry
      - SCHEMA_REGISTRY_KAFKASTORE_SECURITY_PROTOCOL=PLAINTEXT
      - SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=broker:29092
    hostname: schema-registry
    image: confluentinc/cp-schema-registry:7.2.2
    ports:
      - ${DATAHUB_MAPPED_SCHEMA_REGISTRY_PORT:-8081}:8081
  zookeeper:
    container_name: zookeeper
    environment:
      - ZOOKEEPER_CLIENT_PORT=2181
      - ZOOKEEPER_TICK_TIME=2000
    hostname: zookeeper
    image: confluentinc/cp-zookeeper:7.2.2
    ports:
      - ${DATAHUB_MAPPED_ZK_PORT:-2181}:2181
    volumes:
      - zkdata:/var/lib/zookeeper
version: "2.3"
volumes:
  esdata: null
  mysqldata: null
  zkdata: null
```
> python3 -m datahub docker quickstart --start -f docker-compose.yml

### 5 停止datahub
> python3 -m datahub docker quickstart --stop -f docker-compose.yml

##  dinky新增hive2flink任务类型

### 1, 支持执行提交hive sql running on flink

### 2, 测试代码
```
@Test
void testCreateDatabase() {
    sql("create database db1").ok("CREATE DATABASE `DB1`");
    sql("create database db1 comment 'comment db1' location '/path/to/db1'")
            .ok(
                    "CREATE DATABASE `DB1`\n"
                            + "COMMENT 'comment db1'\n"
                            + "LOCATION '/path/to/db1'");
    sql("create database db1 with dbproperties ('k1'='v1','k2'='v2')")
            .ok(
                    "CREATE DATABASE `DB1` WITH DBPROPERTIES (\n"
                            + "  'k1' = 'v1',\n"
                            + "  'k2' = 'v2'\n"
                            + ")");
}
```

### 3, 结果预览

> 测试FlinkHiveSqlParser Passed

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/216624287-ecd671ab-33f0-4ce8-a938-2e4576e21e2b.png">
<br/>

## Flink数据血缘初体验

### 1 结果预览
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/206093771-adfeebf0-ff7d-4044-b3b5-592ff965afa4.png">
<br/>

### 2 创建FlinkDDL

> 参考Resource/FlinkDDLSQL.sql

> CREATE TABLE data_gen (
>
> amount BIGINT
>
> ) WITH (
>
> 'connector' = 'datagen'
>
> 'rows-per-second' = '1'
>
> 'number-of-rows' = '3'
>
> 'fields.amount.kind' = 'random'
>
> 'fields.amount.min' = '10'
>
> 'fields.amount.max' = '11');
>
> CREATE TABLE mysql_sink (
>
> amount BIGINT
>
> PRIMARY KEY (amount) NOT ENFORCED
>
> ) WITH (
>
> 'connector' = 'jdbc'
>
> 'url' = 'jdbc:mysql://localhost:3306/test_db'
>
> 'table-name' = 'test_table'
>
> 'username' = 'root'
>
> 'password' = '123456'
>
> 'lookup.cache.max-rows' = '5000'
>
> 'lookup.cache.ttl' = '10min'
>
> );
>
> INSERT INTO mysql_sink SELECT amount as amount FROM data_gen;

### 3 执行com.platform.FlinkLineageBuild

> 获取结果

> 1, Flink血缘构建结果-表:
>
> [LineageTable{id='4', name='data_gen', columns=[LineageColumn{name='amount', title='amount'}]}
>
> LineageTable{id='6', name='mysql_sink', columns=[LineageColumn{name='amount', title='amount'}]}]
>
> 表ID: 4
>
> 表Namedata_gen
>
> 表ID: 4
>
> 表Namedata_gen
>
> 表-列LineageColumn{name='amount', title='amount'}
>
> 表ID: 6
>
> 表Namemysql_sink
>
> 表ID: 6
>
> 表Namemysql_sink
>
> 表-列LineageColumn{name='amount', title='amount'}
>
> 2, Flink血缘构建结果-边:
>
> [LineageRelation{id='1', srcTableId='4', tgtTableId='6', srcTableColName='amount', tgtTableColName='amount'}]
>
> 表-边: LineageRelation{id='1', srcTableId='4', tgtTableId='6', srcTableColName='amount', tgtTableColName='amount'}

## AllData Doris
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/200178943-7a8edb50-b4d6-4095-9e39-2b3303925701.png">
<br/>

## AllData全新定制一站式场景化大数据中台
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/200179453-bfe6b877-5e59-4239-8217-154a0953c97d.png">
<br/>

## 大数据组件管理DOCKER FOR DATA PLATFORM

### 1, 配置主机服务HOST

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203997670-4b2339e5-d4ba-43ec-afb3-e454646255fd.png">
<br/> 

### 2, 启动大数据集群

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203997598-a80fc4bc-1226-4d7a-9918-39f2586b4170.png">
<br/> 

### 3, YARN正常访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203997781-cb66da01-eddc-4576-b6e0-b107cdaa189b.png">
<br/>

### 4, HIVE正常使用

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203997829-52c8c396-1dc4-4a53-b398-c3a54685f66f.png">
<br/>

### 5, HDFS正常访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998038-0cc73461-6fea-4779-b6c4-0ee293b62832.png">
<br/>

### 6, ES健康检测
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998116-c9cf7a9a-c51e-48d3-823b-9ffef1806567.png">
<br/>

### 7, KIBANA UI访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998431-e8c2a604-d666-4eef-9ea6-b8a2cc669cac.png">
<br/>

### 8, PRESTO UI访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998248-bf6a4090-8943-493b-83a5-87a55d2dcc17.png">
<br/>

### 9, HBASE正常访问

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998514-354d2756-0bee-4cbe-aead-d686fec61da4.png">
<br/>

### 10, FLIKN RUNTIME WEB 正常访问
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203998796-732df150-c697-41b6-a5e7-dc9c65cada4f.png">
<br/>

## 使用Docker/K8S云原生方案-控制各种组件起停

>
> 1, BUSINESS FOR ALL DATA PLATFORM 商业项目
>
> 2, BUSINESS FOR ALL DATA PLATFORM 计算引擎
>
> 3, DEVOPS FOR ALL DATA PLATFORM 运维引擎
>
> 4, DATA GOVERN FOR ALL DATA PLATFORM 数据治理引擎
>
> 5, DATA Integrate FOR ALL DATA PLATFORM 数据集成引擎
>
> 6, AI FOR ALL DATA PLATFORM 人工智能引擎
>
> 7, DATA ODS FOR ALL DATA PLATFORM 数据采集引擎
>
> 8, OLAP FOR ALL DATA PLATFORM OLAP查询引擎
>
> 9, OPTIMIZE FOR ALL DATA PLATFORM 性能优化引擎
>
> 10, DATABASES FOR ALL DATA PLATFORM 分布式存储引擎
>

## DataSophon POC
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/221186382-38f3b419-e710-4b1d-8af9-b176a6d565ac.png">
<br/>

### 一, 项目地址

> https://github.com/datasophon/datasophon
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853673-54092c18-3349-4bcc-b7c7-8b69d5107694.png">
<br/>

### 二, 官方文档

### https://datasophon.github.io/datasophon-website/docs/current/%E6%A6%82%E8%A7%88

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853686-3099ef43-587e-4a64-881a-4e7aceff6441.png">
<br/>

### 三, DataSophon+安装包

#### 提供了一系列包, 可以集成自定义服务
>
> DataSophon+tar.gz安装包, 直接替换Ambari+HDP, 解决大数据集群纳管组件的生命周期问题

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853785-595b760b-5019-4d4f-a6f8-efb5763a49e6.png">
<br/>

### 四, 架构流程图

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853807-1a45f3de-7dba-42fb-80ad-cb729b2ca5ef.png">
<br/>

### 五, 组件自定义服务配置

#### https://datasophon.github.io/datasophon-website/docs/current/%E5%BC%80%E5%8F%91%E8%80%85%E6%8C%87%E5%8D%97/%E7%BB%84%E4%BB%B6%E9%9B%86%E6%88%90/commit_code

### 六, 三件套做监控

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853843-908bd9bd-a716-4ce4-bff1-2fe37956671a.png">
<br/>

### 七, 未来规划

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853889-64ece11e-2c8d-47a7-9c65-8decd4e47635.png">
<br/>

#### 当前版本v1.1.0
>
> 支持系统租户管理
>
> 主机管理支持机架管理
>
> YARN资源调度支持容量调度器
>
> YARN资源调度支持标签调度
>
> 支持组件集成Kerberos, 可自由开启和关闭kerberos认证集成

### 八, 局限

> 版本与代码没有分离, 安装版本的hadoop-3.3.3

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853935-4ed97fe4-1b26-4f56-8a77-7229f91eb850.png">
<br/> 

### 九, 下载, 安装, 解压

> 丝滑切换Ambari Python install, status, start, stop

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853952-a93af1e0-6ac6-4222-8fa3-2b2446fe67ea.png">
<br/> 

### 十, 生态集成

> Dinky, Streampark, Doris等

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220853996-bc592a28-c86f-4bdb-91af-26dcdbf835c7.png">
<br/>

### 十一, 启动逻辑

> 启动DS直接就执行对应的program

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220854013-7d3700d2-f3f9-4ef7-9871-dcc078c69ce7.png">
<br/> 



### 十二, 自定义属性

> 自定义模板+freemaker, 应用自定义属性

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220854025-474a5043-5063-4045-8f1c-9f6e999471e5.png">
<br/> 


### 十三, Actor监听消息通信

> Worker启动并监听Actor失败告警

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/220854046-7a2e1875-ecdc-4de4-be7c-5f1d22ffb319.png">
<br/> 


## Flink Table Store && Lake Storage POC

### 2.1 SQL~Flink table store poc
>
> set execution.checkpointing.interval=15sec;
>
> CREATE CATALOG alldata_catalog WITH (
>
>   'type'='table-store'
>
>   'warehouse'='file:/tmp/table_store'
>
> );
>
> USE CATALOG alldata_catalog;
>
> CREATE TABLE word_count (
>
>     word STRING PRIMARY KEY NOT ENFORCED
>     
>     cnt BIGINT
>
> );
>
> CREATE TEMPORARY TABLE word_table (
>
>     word STRING
>
> ) WITH (
>
>     'connector' = 'datagen'
>     
>     'fields.word.length' = '1'
>
> );
>
> INSERT INTO word_count SELECT word, COUNT(*) FROM word_table GROUP BY word;
>
> -- POC Test OLAP QUERY
>
> SET sql-client.execution.result-mode = 'tableau';
>
> RESET execution.checkpointing.interval;
>
> SET execution.runtime-mode = 'batch';
>
> SELECT * FROM word_count;
>
> -- POC Test Stream QUERY
>
> -- SET execution.runtime-mode = 'streaming';
>
> -- SELECT `interval`, COUNT(*) AS interval_cnt FROM
>
> --   (SELECT cnt / 10000 AS `interval` FROM word_count) GROUP BY `interval`;

### 2.2 Flink Runtime Web
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203073679-f64b4655-7ea8-4c36-98ab-7b1806119224.png">
<br/>

### 2.3 Flink Batch
>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203073715-e69d8378-1b37-4fea-851f-9f3e6a9d62eb.png">
<br/>

### 2.4 Flink Olap Read
>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203073740-e088e842-3010-42af-bfc2-0808d5e1940f.png">
<br/> 

### 2.5 Flink Stream Read
>
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203073760-906f0b1c-498b-4713-931b-25a90f53e985.png">
<br/> 

## Dlink二开新增Flink1.16.0支持
### 1, Dlink配置Flink Table Store相关依赖
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467342-fd24f652-2fb5-4e4e-9b6e-23a113817b6b.png">
<br/> 
### 2, Dlink启动并运行成功
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467422-a2c39226-31c6-4998-a926-71068b36de4d.png">
<br/> 
### 3, OLAP查询
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467379-b498da92-218f-4f29-a977-dd17fa374ea0.png">
<br/> 

### 4, Flink1.16.0 Dlink流式读
> 4.1 Stream Read 1
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467499-e1541c84-f8c8-40ff-aa33-cdd35cae2932.png">
<br/>
> 4.2 Stream Read 2
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/203467519-83fd40d5-823d-45b0-8cdd-09e0b9b09cb2.png">
<br/>


## Architecture
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171598215-0914f665-9950-476c-97ff-e7e07aa10eaf.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171598333-d14ff53f-3af3-481c-9f60-4f891a535b5c.png">
<br/>

| Component                                                                       | Description                                                    | Important Composition       |
|---------------------------------------------------------------------------------|----------------------------------------------------------------|-----------------------------|
| [**ai**](https://github.com/alldatacenter/alldata/tree/master/ai)               | AI STUDIO FOR ALL DATA PLATFORM artificial intelligence engine | 人工智能引擎                      |
| [**assembly**](https://github.com/alldatacenter/alldata/tree/master/assembly)   | WHOLE PACKAGE BUILD FOR ALL DATA PLATFORM assembly engine      | 整包构建引擎                      |
| [**cluster**](https://github.com/alldatacenter/alldata/tree/master/cluster)     | DATA SRE FOR ALL DATA PLATFORM OLAP query engine               | 智能大数据运维引擎                   |
| [**crawlab**](https://github.com/alldatacenter/alldata/tree/master/crawlab)     | CRAWLER PLATFORM FOR ALL DATA PLATFORM commerce engine         | 爬虫引擎系统                      |
| [**document**](https://github.com/alldatacenter/alldata/tree/master/document)   | DOCUMENT FOR ALL DATA PLATFORM OLAP query engine               | 官方文档                        |
| [**dts**](https://github.com/alldatacenter/alldata/tree/master/dts)             | DTS FOR ALL DATA PLATFORM DATA DTS engine                      | 数据集成引擎                      |
| [**factory**](https://github.com/alldatacenter/alldata/tree/master/factory)         | FACTORY FOR ALL DATA PLATFORM DATA DTS engine                  | 数据工厂引擎                      |
| [**fs**](https://github.com/alldatacenter/alldata/tree/master/fs)               | DATA STORAGE FOR ALL DATA PLATFORM DATA STORAGE engine         | 大数据存储引擎                     |
| [**govern**](https://github.com/alldatacenter/alldata/tree/master/govern)       | DATA GOVERN FOR ALL DATA PLATFORM Data Governance Engine       | 数据治理引擎                      |
| [**iot**](https://github.com/alldatacenter/alldata/tree/master/iot)             | IOT FOR ALL DATA PLATFORM Data Governance Engine               | 云原生IOT开发框架                  |
| [**kg**](https://github.com/alldatacenter/alldata/tree/master/kg)               | KNOWLEDGE GRAPH FOR ALL DATA PLATFORM Data Task Engine         | 知识图谱引擎                      |
| [**lakehouse**](https://github.com/alldatacenter/alldata/tree/master/lakehouse) | ONE LAKE FOR ALL DATA PLATFORM ONE LAKE engine                 | 数据湖引擎                       |
| [**market**](https://github.com/alldatacenter/alldata/tree/master/market)       | MARKET FOR ALL DATA PLATFORM MARKET engine                     | 数据实验场引擎                     |
| [**olap**](https://github.com/alldatacenter/alldata/tree/master/olap)           | OLAP FOR ALL DATA PLATFORM OLAP query engine                   | 混合OLAP查询引擎                  |
| [**trade**](https://github.com/alldatacenter/alldata/tree/master/trade)         | TRADE FOR ALL DATA PLATFORM TRADE Engine                       | TRADE引擎                     |
| [**wiki**](https://github.com/alldatacenter/alldata/tree/master/wiki)           | WIKI FOR ALL DATA PLATFORM WIKI Engine                         | AllData知识库                  |
| [**alldata**](https://github.com/alldatacenter/alldata)                         | AllData社区项目通过二开大数据生态组件, 以及大数据采集, 大数据存储, 大数据计算, 大数据开发来建设一站式大数据平台    | Github一站式开源大数据平台AllData社区项目 |


## AllData社区商业计划图
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/188898972-d78bcbb6-eb30-420d-b5e1-7168aa340555.png">
<br/>

## AllData社区项目业务流程图
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/188899006-aba25703-f8fa-42b6-b59f-2573ee2b27fc.png">
<br/>

## AllData社区项目树状图
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/188898939-bfba6cbc-c7b0-40c4-becd-27152d5daa90.png">
<br/>

## 全站式AllData产品路线图
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/179927878-ff9c487e-0d30-49d5-bc88-6482646d90a8.png">
<br/>


## AllData社区项目时间旅行
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/188899033-948583a4-841b-4233-ad61-bbc45c936ca1.png">
<br/>

## 实时推荐系统业务流程图
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/191244864-3cffc8e3-e41e-4865-8b2b-376742f10a8e.png">
<br/>

## AllData总部前后端解决方案
### 包括AllData前后端解决方案, 多租户运维平台前后端
### 基于`eladmin` + `tenant` 建设AllData前后端解决方案

> 1, AllData前端解决方案 `studio/eladmin-web`
>
> 2, AllData后端解决方案 `studio/eladmin`
>
> 3, 多租户运维平台前端 `studio/tenant`
>
> 4, 多租户运维平台前端 `studio/tenantBack`

<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/196594418-1ba618cb-da53-487a-951d-0715e3fc685e.jpg">


## Integration

## Data Quality
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132004-542b699c-2878-4648-a79e-f118f28a0ed2.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171131705-86a2c0bd-cd9d-4a66-b209-5c41d1b18e56.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132036-613e1271-d122-47dc-af7c-a3ee2a203a2e.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132186-261b742a-dc88-4739-8327-08b503fce8d8.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132633-193bbba4-58d6-4b38-8e9e-4674cdfa7cdd.png">
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171132684-37ebcec6-05dd-45d6-83cd-d4f18416b755.png"> 
<br/>

<br/>

### Livy访问查看JOB

<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171131636-4cb6d93b-c994-4dfa-bfee-48d2a04c4963.png">  
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/171133364-8e4a8e84-c9f9-456c-9f33-c90b90cf54e4.png"> 
<br/>

## 离线商城数仓展示
<br>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160219586-e2e190fa-21f6-4f87-bbbc-7cdd6ecc625a.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160221446-24d9438d-703c-4d17-880e-5d34d0f8d229.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160221463-772477c8-f996-45df-ab74-9e7a179adc81.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160220078-bdabde8b-9467-4d26-8675-37712e1d48b1.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160220090-d5c33c1f-9507-4338-98e1-0abc29c4dbad.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160220100-83391805-29ee-45d2-8076-f743c3ba6070.png">
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/160220106-0341a2f4-b4df-4d2b-9ec1-b0f10affd22d.png">
<br/>

## 知识图谱建设方法论
<br/>
<img width="1215" alt="image" src="https://user-images.githubusercontent.com/20246692/218489554-8c49659f-1c52-4b67-9d8d-671d85191d66.png">
<br/>
## 知识图谱（Knowledge Graph）

## 知识图谱建设方法论:
### 一, 知识图谱技术架构: 确定知识的表示方式和知识的存储方式, 

### 二, 知识图谱建设方法论: 知识图谱建设可以分为知识建模, 知识抽取, 知识验证这样几个阶段, 形成一个知识图谱
>
>  从知识抽取的内容上, 又可以分为实体抽取, 属性抽取, 关系抽取, 事件抽取:
>
> 实体抽取指从数据源中检测到可命名的实体, 并将它们分类到已建模的类型中, 例如人, 组织, 地点, 时间等等, 
>
> 属性抽取是识别出命名实体的具体属性, 
>
> 关系抽取是识别出实体与实体之间的关系, 例如从句子“著名歌手周杰伦的妻子昆凌”中识别出“周杰伦”与“昆凌”之间的夫妻关系, 
>
> 事件抽取是识别出命名实体相关的事件信息, 例如“周杰伦”与“昆凌”结婚就是一个事件
>
> 可以看出实体抽取, 属性抽取, 关系抽取是抽取我们在知识建模中定义的拓扑结构部分数据
>
> 事件抽取是事件建模相关数据的抽取, 所以在领域知识图谱建设中, 也需要包括数据准备域的抽取方式, 处置域的数据抽取方式
>
> 知 识 验 证
>
> 从各种不同数据源抽取的知识, 并不一定是有效的知识, 必须进行知识的验证, 将有效的, 正确的知识进入知识库造成知识不准确的原因
>
> 通常是原始数据存在错误, 术语存在二义性, 知识冲突等等, 例如前面提到的"1#"压水堆, "1号"压水堆, “一号”压水堆这三个词对应一个实体
>
> 如果在抽取中没有合理定义规则, 这就需要在知识验证阶段得到处理, 以便形成闭环

### 三, 基于知识图谱建设应用: 每一类应用的侧重点不同, 使用技术和达到的效果也不同, 我们总结为知识推理类, 知识呈现类, 知识问答类, 知识共享类

> 1, 知识图谱建设
>
> 1.1 人工数据标注工具: https://github.com/doccano/doccano
>
> 1.2 自动标注+知识抽取: https://github.com/zjunlp/DeepKE
>
> 2, 知识存储: https://github.com/alibaba/GraphScope
>
> 3, 知识图谱应用: https://github.com/lemonhu/stock-knowledge-graph


## 从0到1建设大数据解决方案

> 从0到1建设大数据解决方案是一个相对比较宏观的过程, 需要考虑从业务需求分析, 数据采集, 数据处理, 数据存储, 数据查询分析到数据可视化展示等多个环节, 
>
> 以下是一个简单的大数据解决方案建设方法论: 
>
> 需求分析: 首先需要明确业务需求, 包括数据源, 数据量, 数据类型, 数据质量等等, 可以与业务人员进行沟通, 制定出明确的需求和目标, 确定解决方案的规模和数据的范围
>
> 数据采集: 根据需求分析结果, 确定数据来源和采集方式, 可以使用采集工具或者开发自定义采集程序, 采集的数据需要进行清洗和过滤, 确保数据的准确性和完整性
>
> 数据处理: 数据采集后需要进行清洗, 整合, 加工等处理, 以便后续的存储和分析, 数据处理可以使用数据流处理或者批处理等方式
>
> 数据存储: 对于大数据解决方案, 数据存储是一个非常重要的环节, 需要选择合适的存储方案, 包括分布式存储, 列式存储, 内存数据库等, 可以根据数据量和查询分析方式等要素进行选择
>
> 数据查询分析: 建立数据查询和分析体系, 需要考虑数据查询和分析的灵活性和效率, 可以使用数据查询引擎和分析工具, 如Hadoop, Spark, Hive, Presto, Superset等
>
> 数据可视化展示: 通过数据可视化展示方式, 使数据分析结果直观, 易于理解, 可以使用开源的可视化工具, 如Tableau, Power BI, Echarts等
>
> 安全与隐私: 对于大数据解决方案, 安全和隐私是非常重要的, 需要采取一系列的安全措施, 包括数据加密, 访问控制, 数据备份等, 以保障数据的安全性和隐私性
>
> 评估和优化: 在建设过程中需要不断评估和优化解决方案, 调整方案架构和技术选型, 以提高解决方案的性能和效率, 满足业务需求和用户期望
>
> 以上是一个简单的从0到1建设大数据解决方案的方法论, 需要根据实际情况进行具体的调整和优化

## 数字化转型
>
> 数字化转型是指将传统企业在信息化, 网络化, 智能化, 数据化等技术的支撑下, 对业务, 组织, 文化, 价值创造, 利益分配等方面进行全面的革新和升级, 
>
> 以适应市场, 技术, 用户等环境的变化数字化转型的目标是实现企业从传统生产经营方式向数字化经营模式的转变, 提高企业的效率, 创新能力, 市场竞争力和盈利能力
>
> 数字化转型方法论可以概括为以下几个方面: 
>
> 确定数字化转型的战略目标和方向, 明确数字化转型的意义和价值, 为数字化转型的实施提供方向和支撑
>
> 分析业务过程, 识别业务痛点和机会, 确定数字化转型的重点领域和项目, 以提高效率, 创新能力和用户体验为导向
>
> 优化组织结构和流程, 建立数字化组织架构和工作流程, 激发组织创新和员工动力, 提高业务效率和创新能力
>
> 采用先进的信息技术和数据技术, 例如云计算, 大数据, 人工智能, 物联网等, 为数字化转型提供技术支持
>
> 建立数字化文化, 通过数字化营销, 数字化服务, 数字化协同等方式, 提升品牌价值, 用户满意度和市场影响力
>
> 实施数字化监管, 建立数字化安全, 合规和风险控制体系, 确保数字化转型的合法性, 合规性和可持续性
>
> 数字化转型是一个复杂的过程, 需要综合运用战略, 组织, 技术, 文化, 监管等多方面的手段和方法, 才能取得成功



## Community

> 联系作者: https://docs.qq.com/doc/DVFVMYUp6cFhSRVJs
