name: CI with flink cdc Test

on:
  push:
    branches: [ "main", "release/spark_3.3" ]

jobs:
  build:

    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v3
      - name: Set up JDK 8
        uses: actions/setup-java@v3
        with:
          java-version: '8'
          distribution: 'temurin'
          cache: maven
      - name: Set up Python 3.9
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip setuptools wheel
          pip install pymysql cryptography jproperties --no-cache-dir
          sudo apt-get install -y git-lfs
      - name: Git LFS checkout
        run: |
          git lfs fetch
          git lfs checkout
      - uses: actions-rs/toolchain@v1
        with:
          toolchain: nightly
      - name: Set up cargo cache
        uses: actions/cache@v3
        continue-on-error: false
        with:
          path: |
            ~/.cargo/bin/
            ~/.cargo/registry/index/
            ~/.cargo/registry/cache/
            ~/.cargo/git/db/
            native-io/target/
          key: ${{ runner.os }}-cargo-${{ hashFiles('native-io/Cargo.lock') }}
          restore-keys: ${{ runner.os }}-cargo-
      - name: Pull images
        run: |
          docker pull -q bitnami/spark:3.3.1
          docker pull -q dockcross/manylinux2014-x64:latest
      - name: Build with Maven
        run: |
          MAVEN_OPTS="-Xmx4000m" mvn -q -B clean package -f pom.xml -DskipTests
      - name: Get jar names
        run: |
          echo "FLINK_JAR_NAME=$(python script/get_jar_name.py lakesoul-flink)" >> $GITHUB_ENV
          echo "SPARK_JAR_NAME=$(python script/get_jar_name.py lakesoul-spark)" >> $GITHUB_ENV
          echo "SPARK_TEST_JAR_NAME=$(python script/get_jar_name.py lakesoul-spark | sed -e 's/.jar/-tests.jar/g')" >> $GITHUB_ENV
      - name: Copy built jar to work-dir
        run: |
          cp ./lakesoul-flink/target/$FLINK_JAR_NAME ./script/benchmark/work-dir
          cp ./lakesoul-spark/target/$SPARK_JAR_NAME ./script/benchmark/work-dir
          cp ./lakesoul-spark/target/$SPARK_TEST_JAR_NAME ./script/benchmark/work-dir
      - name: Deploy cluster
        run: |
          cd ./docker/lakesoul-docker-compose-env
          docker compose pull -q
          docker compose up -d
          sleep 10s
      - name: Start flink task
        run: |
          docker exec -t lakesoul-docker-compose-env-jobmanager-1 flink run -d -c org.apache.flink.lakesoul.entry.MysqlCdc /opt/flink/work-dir/$FLINK_JAR_NAME --source_db.host mysql --source_db.port 3306 --source_db.db_name test_cdc --source_db.user root --source_db.password root --source.parallelism 2 --sink.parallelism 4 --warehouse_path s3://lakesoul-test-bucket/data/ --flink.checkpoint s3://lakesoul-test-bucket/chk --flink.savepoint s3://lakesoul-test-bucket/svp --job.checkpoint_interval 10000 --server_time_zone UTC
          sleep 20s
      - name: Download mysql driver jar
        run: |
          cd ./script/benchmark/work-dir
          if [ ! -e mysql-connector-java-8.0.30.jar ]; then wget -q https://repo1.maven.org/maven2/mysql/mysql-connector-java/8.0.30/mysql-connector-java-8.0.30.jar; fi
      - name: Create table and insert data
        run: |
          cd ./script/benchmark
          python 1_create_table.py
          docker exec -i lakesoul-docker-compose-env-mysql-1 bash /2_insert_table_data.sh
          sleep 1m
      - name: Accuracy verification task
        run: |
          cd ./script/benchmark
          docker run --cpus 2 -m 5000m --net lakesoul-docker-compose-env_default --rm -t -v ${PWD}/work-dir:/opt/spark/work-dir --env lakesoul_home=/opt/spark/work-dir/lakesoul.properties bitnami/spark:3.3.1 spark-submit --driver-memory 4G --executor-memory 4G --conf spark.driver.memoryOverhead=1500m --conf spark.executor.memoryOverhead=1500m --jars /opt/spark/work-dir/$SPARK_JAR_NAME,/opt/spark/work-dir/mysql-connector-java-8.0.30.jar --conf spark.hadoop.fs.s3.buffer.dir=/tmp --conf spark.hadoop.fs.s3a.buffer.dir=/tmp --conf  spark.hadoop.fs.s3a.fast.upload.buffer=disk --conf spark.hadoop.fs.s3a.fast.upload=true --class org.apache.spark.sql.lakesoul.benchmark.Benchmark --master local[4] /opt/spark/work-dir/$SPARK_TEST_JAR_NAME
      - name: Adding columns for tables and deleting some data from tables
        run: |
          cd ./script/benchmark
          python3 3_add_column.py
          python3 delete_data.py
          docker exec -i lakesoul-docker-compose-env-mysql-1 bash /2_insert_table_data.sh
          sleep 1m
      - name: Accuracy verification task
        run: |
          cd ./script/benchmark
          docker run --cpus 2 -m 5000m --net lakesoul-docker-compose-env_default --rm -t -v ${PWD}/work-dir:/opt/spark/work-dir --env lakesoul_home=/opt/spark/work-dir/lakesoul.properties bitnami/spark:3.3.1 spark-submit --driver-memory 4G --executor-memory 4G --conf spark.driver.memoryOverhead=1500m --conf spark.executor.memoryOverhead=1500m --jars /opt/spark/work-dir/$SPARK_JAR_NAME,/opt/spark/work-dir/mysql-connector-java-8.0.30.jar --class org.apache.spark.sql.lakesoul.benchmark.Benchmark --master local[4] /opt/spark/work-dir/$SPARK_TEST_JAR_NAME
      - name: Updating data in tables
        run: |
          cd ./script/benchmark
          python3 4_update_data.py
          sleep 1m
      - name: Accuracy verification task
        run: |
          cd ./script/benchmark
          docker run --cpus 2 -m 5000m --net lakesoul-docker-compose-env_default --rm -t -v ${PWD}/work-dir:/opt/spark/work-dir --env lakesoul_home=/opt/spark/work-dir/lakesoul.properties bitnami/spark:3.3.1 spark-submit --driver-memory 4G --executor-memory 4G --conf spark.driver.memoryOverhead=1500m --conf spark.executor.memoryOverhead=1500m --jars /opt/spark/work-dir/$SPARK_JAR_NAME,/opt/spark/work-dir/mysql-connector-java-8.0.30.jar --class org.apache.spark.sql.lakesoul.benchmark.Benchmark --master local[4] /opt/spark/work-dir/$SPARK_TEST_JAR_NAME
      - name: Dropping columns and deleting some data in tables
        run: |
          cd ./script/benchmark
          python3 6_drop_column.py
          python3 delete_data.py
          docker exec -i lakesoul-docker-compose-env-mysql-1 bash /2_insert_table_data.sh
          sleep 1m
      - name: Accuracy verification task
        run: |
          cd ./script/benchmark
          docker run --cpus 2 -m 5000m --net lakesoul-docker-compose-env_default --rm -t -v ${PWD}/work-dir:/opt/spark/work-dir --env lakesoul_home=/opt/spark/work-dir/lakesoul.properties bitnami/spark:3.3.1 spark-submit --driver-memory 4G --executor-memory 4G --conf spark.driver.memoryOverhead=1500m --conf spark.executor.memoryOverhead=1500m --jars /opt/spark/work-dir/$SPARK_JAR_NAME,/opt/spark/work-dir/mysql-connector-java-8.0.30.jar --class org.apache.spark.sql.lakesoul.benchmark.Benchmark --master local[4] /opt/spark/work-dir/$SPARK_TEST_JAR_NAME