#/*
 # * Licensed to the Apache Software Foundation (ASF) under one or more
 # * contributor license agreements.  See the NOTICE file distributed with
 # * this work for additional information regarding copyright ownership.
 # * The ASF licenses this file to You under the Apache License, Version 2.0
 # * (the "License"); you may not use this file except in compliance with
 # * the License.  You may obtain a copy of the License at
 # *
 # *    http://www.apache.org/licenses/LICENSE-2.0
 # *
 # * Unless required by applicable law or agreed to in writing, software
 # * distributed under the License is distributed on an "AS IS" BASIS,
 # * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # * See the License for the specific language governing permissions and
 # * limitations under the License.
 # */
Jdbc {
    url = "JDBC连接Url"
    url_description = "数据库的JDBC连接地址. 比如 jdbc:mysql://localhost:3306/test?enabledTLSProtocols=TLSv1.2&rewriteBatchedStatements=true"
    driver = "JDBC驱动名"
    driver_description = "数据库JDBC驱动名. 比如MySQL的驱动: com.mysql.cj.jdbc.Driver"
    user = "用户名"
    password = "密码"
    query = "查询SQL"
    query_description = "从源读取数据的sql"
    connection_check_timeout_sec = "JDBC连接超时时长"
    connection_check_timeout_sec_description = "JDBC连接超时时长"
    partition_column = "并行读取拆分使用的字段"
    partition_column_description = "并行读取拆分使用的字段，建议使用表中的自增主键或足够散列数字类型主键字段，目前只支持单个字段"
    partition_upper_bound = "拆分字段上边界"
    partition_upper_bound_description = "从源表中查询数据时将会自动添加 where xxx <= n 语句，其中xxx为并行读取拆分字段，n 为这里设置的拆分字段上边界值"
    partition_lower_bound = "拆分字段下边界"
    partition_lower_bound_description = "从源表中查询数据时将会自动添加 where xxx >= m 语句，其中xxx为并行读取拆分字段，m 为这里设置的拆分字段下边界值"
    partition_num = "拆分的分片数"
    partition_num_description = "将读取数据拆分成多少个分片，将基于拆分字段的上下边界除以该值计算每个分片的步长"
    fetch_size = "fetch size"
    fetch_size_description = "JDBC每次检索结果集取出的行数"
    parallelism = "并行度"
    parallelism_description = "用于读取数据的线程数"
    database = "数据库"
    table = "表名"
    primary_keys = "主键列"
    primary_keys_description = "目标表的主键字段列表，[key1, key2, ...]"
    support_upsert_by_query_primary_key_exist = "基于主键做upsert"
    support_upsert_by_query_primary_key_exist_description = "基于主键做upsert"
    max_retries = "最大重试次数"
    batch_interval_ms = "两次批量写入最大间隔时长"
    batch_interval_ms_description = "两次批量写入最大间隔时长"
    batch_size = "每次批量写入最大行数"
    batch_size_description = "每次批量写入最大行数"
    is_exactly_once = "数据一致性保证"
    is_exactly_once_description = "是否开启目标端数据库的XA事务来保证数据的一致性"
    xa_data_source_class_name = "XA事务的名称"
    xa_data_source_class_name_description = "XA事务名称"
    max_commit_attempts = "XA事务提交最大重试次数"
    max_commit_attempts_description = "XA事务提交最大重试次数"
    transaction_timeout_sec = "事务提交超时时长"
    transaction_timeout_sec_description = "事务提交超时时长"
    auto_commit = "是否自动提交事务"
    auto_commit_description = "是否自动提交事务"
}

S3File {
    path = "路径"
    path_description = "读取或写入数据的路径，例如: /seatunnel/path1"
    file_format_type = "文件格式"
    bucket = "S3桶地址"
    fs_s3a_endpoint = "S3a Endpoint"
    fs_s3a_endpoint_description = "例如: s3.cn-north-1.amazonaws.com.cn"
    fs_s3a_aws_credentials_provider = "S3 Credentials Provider"
    read_columns = "读取的字段列表"
    read_columns_description = "需要从文件中读取的字段列表，应该是文件中所有字段的子集"
    access_key = "Access Key"
    access_secret = "Access Secret"
    hadoop_s3_properties = "Hadoop S3 Api可选参数"
    hadoop_s3_properties_description = "使用hadoop s3 api访问s3可选的其它参数. 比如: \n{'{'}fs.s3a.buffer.dir = /data/st_test/s3a\nfs.s3a.fast.upload.buffer = disk\n{'}'}"
    delimiter = "文件中的字段分隔符"
    delimiter_description = "文件中使用的字段分隔符, 默认值是 \\001"
    parse_partition_from_path = "是否从子目录中解析分区字段"
    parse_partition_from_path_description = "是否从子目录中解析分区字段"
    date_format = "日期格式化"
    date_format_description = "日期类型的格式化，告诉同步任务该如何将字符串转换成日志，目前支持：yyyy-MM-dd， yyyy.MM.dd， yyyy/MM/dd。 默认值是 yyyy-MM-dd"
    datetime_format = "日期时间格式化"
    datetime_format_description = "Datetime类型格式，用于告诉同步任务如何将字符串转换为Datetime，支持以下格式: yyyy-MM-dd HH:mm:ss yyyy.MM.dd HH:mm:ss yyyy/MM/dd HH:mm:ss yyyyMMddHHmmss. 默认值是 yyyy-MM-dd HH:mm:ss"
    time_format = "时间类型格式化"
    time_format_description = "时间类型格式，用于告诉连接器如何将字符串转换为时间，支持以下格式: HH:mm:ss HH:mm:ss.SSS. 默认值是 HH:mm:ss"
    skip_header_row_number = "跳过文件的前几行"
    skip_header_row_number_description = "跳过前几行，仅用于text和csv文件格式类型。"
    schema = "schema"
}

MySQL-CDC {
    username = "用户名"
    password = "密码"
    base_url = "JDBC连接Url"
    server_time_zone = "数据库服务器会话时区"
    format = "数据输出格式"
    DEFAULT = "Default"
    COMPATIBLE_DEBEZIUM_JSON = "Compatible Debezium Json"
    snapshot_split_size = "历史数据阶段每次读取的行数"
    snapshot_split_size_description = "在历史数据读取阶段，每次Split读取的数据行数。每个表将被拆分为多个Split以进行并行读取"
    snapshot_fetch_size = "JDBC Fetch Size"
    snapshot_fetch_size_description = "读取表快照时的JDBC Fetch Size"
    incremental_parallelism = "增量读取阶段并行度"
    incremental_parallelism_description = "增量读取状态下的并行数。现在仅支持配置为1"
    startup_mode = "作业启动模式"
    INITIAL = "先全量同步所有历史数据，再自动增量同步新增数据"
    EARLIEST = "不同步历史数据，从能找到的最早的binlog开始增量同步"
    LATEST = "不同步历史数据，从能找到的最新的binlog开始增量同步"
    SPECIFIC = "不同步历史数据，从指定的binlog文件和位置开始增量同步"
    TIMESTAMP = "不同步历史数据，从指定的时间查找对应的binlog文件和位置开始增量同步"
    startup_specific_offset_file = "开始同步的log文件名"
    startup_specific_offset_file_description = "配置文件和位置以开始读取"
    startup_specific_offset_pos = "开始同步的log位置"
    startup_specific_offset_pos_description = "配置开始读取的log的位置"
    stop_mode = "作业停止模式"
    NEVER = "持续运行，不主动停止"
    debezium = "Debezium相关的配置参数"
    debezium_description = "将Debezium的参数传递给DebeziumEmbeddedEngine，该引擎用于从MySQL服务器捕获数据更改。您可以从 https://debezium.io/documentation/reference/1.6/connectors/mysql.html#mysql-connector-properties 获取更多信息. 示例: \n{'{'}\n snapshot.mode = \"never\"\n decimal.handling.mode = \"double\"\n:{'}'}"
    server_id = "MySQL Server Id"
    connect_timeout_ms = "数据库连接超时时长（毫秒）"
    connect_timeout_ms_description = "在超时之前，连接器尝试连接到数据库服务器后应等待的最长时间"
    connect_max_retries = "数据库连接最大重试次数"
    connect_max_retries_description = "连接器可以重试以建立数据库服务器连接的最大重试次数"
    connection_pool_size = "数据库连接池大小"
    connection_pool_size_description = "数据库连接池大小"
    parallelism = "并行度"
    parallelism_description = "读取历史数据阶段的并行度。它影响读取历史数据的速度。"
}

SqlServer-CDC {
    username = "用户名"
    password = "密码"
    base_url = "JDBC连接Url"
    server_time_zone = "数据库服务器会话时区"
    format = "数据输出格式"
    DEFAULT = "Default"
    COMPATIBLE_DEBEZIUM_JSON = "Compatible Debezium Json"
    snapshot_split_size = "历史数据阶段每次读取的行数"
    snapshot_split_size_description = "在历史数据读取阶段，每次Split读取的数据行数。每个表将被拆分为多个Split以进行并行读取"
    snapshot_fetch_size = "JDBC Fetch Size"
    snapshot_fetch_size_description = "读取表快照时的JDBC Fetch Size"
    incremental_parallelism = "增量读取阶段并行度"
    incremental_parallelism_description = "增量读取状态下的并行数。现在仅支持配置为1"
    startup_mode = "作业启动模式"
    INITIAL = "先全量同步所有历史数据，再自动增量同步新增数据"
    EARLIEST = "不同步历史数据，从能找到的最早的binlog开始增量同步"
    LATEST = "不同步历史数据，从能找到的最新的binlog开始增量同步"
    SPECIFIC = "不同步历史数据，从指定的binlog文件和位置开始增量同步"
    TIMESTAMP = "不同步历史数据，从指定的时间查找对应的binlog文件和位置开始增量同步"
    startup_specific_offset_file = "开始同步的log文件名"
    startup_specific_offset_file_description = "配置文件和位置以开始读取"
    startup_specific_offset_pos = "开始同步的log位置"
    startup_specific_offset_pos_description = "配置开始读取的log的位置"
    stop_mode = "作业停止模式"
    NEVER = "持续运行，不主动停止"
    debezium = "Debezium相关的配置参数"
    debezium_description = "将Debezium的参数传递给DebeziumEmbeddedEngine，该引擎用于从MySQL服务器捕获数据更改。您可以从 https://debezium.io/documentation/reference/1.6/connectors/mysql.html#mysql-connector-properties 获取更多信息. 示例: \n{'{'}\n snapshot.mode = \"never\"\n decimal.handling.mode = \"double\"\n{'}'}"
    server_id = "MySQL Server Id"
    connect_timeout_ms = "数据库连接超时时长（毫秒）"
    connect_timeout_ms_description = "在超时之前，连接器尝试连接到数据库服务器后应等待的最长时间"
    connect_max_retries = "数据库连接最大重试次数"
    connect_max_retries_description = "连接器可以重试以建立数据库服务器连接的最大重试次数"
    connection_pool_size = "数据库连接池大小"
    connection_pool_size_description = "数据库连接池大小"
    parallelism = "并行度"
    parallelism_description = "读取历史数据阶段的并行度。它影响读取历史数据的速度。"
}

Elasticsearch {
    hosts = "访问地址"
    hosts_description = "ElasticSearch中HTTP端口信息，类型为List，如：[\"127.0.0.1:9200\",\"127.0.0.2:9200\"]"
    username = "用户名"
    password = "密码"
    tls_verify_certificate = "为HTTPS端点启用证书验证"
    tls_verify_certificate_description = "为HTTPS端点启用证书验证"
    tls_verify_hostname = "为HTTPS端点启用主机名验证"
    tls_verify_hostname_description = "为HTTPS端点启用主机名验证"
    tls_keystore_path = "PEM或JKS密钥库的路径"
    tls_keystore_path_description = "PEM 或 JKS 密钥库的路径。运行 WhaleTunnel 的操作系统用户必须可以读取该文件。"
    tls_keystore_password = "密钥库的密钥密码"
    tls_keystore_password_description = "指定密钥库的密钥密码"
    tls_truststore_path = "PEM或JKS信任库的路径"
    tls_truststore_path_description = "PEM 或 JKS 信任库的路径。运行 WhaleTunnel 的操作系统用户必须可以读取该文件。"
    tls_truststore_password = "信任库的密钥密码"
    scroll_time = "Scroll Time"
    scroll_time_description = "Elasticsearch 将为滚动请求保持搜索上下文活动的时间大小"
    scroll_size = "Scroll Size"
    scroll_size_description = "每个 Elasticsearch 滚动请求返回的最大命中数"
    parallelism = "并行度"
    parallelism_description = "并行度，并行度越大读取速度越快，消耗的机器资源越多"
    index_type = "索引类型"
    index_type_description = "Elasticsearch索引类型，建议在elasticsearch 6及以上不要指定"
    primary_keys = "主键字段"
    primary_keys_description = "用于生成文档_id 的主键字段，这是CDC模式（例如Source为MySQL-CDC） 必需的选项."
    key_delimiter = "主键分隔符"
    key_delimiter_description = "复合键的分隔符（默认为\"_\"），例如，\"$\"将生成文档 _id \"KEY1$KEY2$KEY3\""
    max_retry_count = "写入请求最大重试次数"
    max_batch_size = "批量写入一次最大数据量"
    max_batch_size_description = "批量写入请求的最大数据大小"
}

Kafka {
    bootstrap_servers = "连接地址"
    bootstrap_servers_description = "Kafka的集群地址，例如：host1:9092, host2:9092"
    kafka_config = "Kafka配置信息"
    kafka_config_description = "用于输入一些额外的参数，使用HOCON格式，例如\n{'{'}\n max.pool.recodes=500\n{'}'}"
    topic = "topic"
    pattern = "是否正则匹配"
    start_mode = "启动模式"
    EARLIEST = "从最早的Offset开始消费"
    GROUP_OFFSETS = "从消息者Group中保存的Offset开始消费"
    LATEST = "从最新的Offset开始消费"
    TIMESTAMP = "从指定的时间开始消费"
    SPECIFIC_OFFSETS = "从指定的Offset开始消费"
    consumer_group = "消费者Group"
    consumer_group_description = "消费组，默认为SeaTunnel-Consumer-Group"
    commit_on_checkpoint = "Commit On Checkpoint"
    commit_on_checkpoint_description = "触发Checkpoint的时候是否提交Offset"
    format = "数据格式"
    partition_discovery_interval_millis = "查询分区的时间间隔"
    partition_discovery_interval_millis_description = "查询分区的时间间隔，如果设置大于0则可以在运行时动态的发现分区。"
    parallelism = "并行度"
}

StarRocks {
    nodeUrls = "Http连接地址"
    nodeUrls_description = "StarRocks中HTTP端口信息，类型为List，如：[\"127.0.0.1:8030\"]"
    username = "Username"
    password = "Password"
    base_url = "JDBC连接地址"
    base_url_description = "用于定义连接StarRocks的JDBC URL，如：jdbc:mysql://127.0.0.1:9030/test"
    labelPrefix = "Label Prefix"
    labelPrefix_description = "用于指定导入作业的标签。如果您不指定标签，StarRocks 会自动为导入作业生成一个标签。相同标签的数据无法多次成功导入，这样可以避免一份数据重复导入。有关标签的命名规范，请参见系统限制。StarRocks 默认保留最近 3 天内成功的导入作业的标签。您可以通过 FE 配置参数 label_keep_max_second 设置默认保留时长。"
    batch_max_rows = "批量写入最大行数"
    batch_max_bytes = "批量写入最大字节数"
    batch_interval_ms = "两个批次写入之间最大间隔时长"
    max_retries = "最大重试次数"
    max_retry_backoff_ms = "两次重试之间最大间隔时长"
    retry_backoff_multiplier_ms = "重试间隔时长增长系数"
    enable_upsert_delete = "启用Upsert/Delete"
    enable_upsert_delete_description = "是否开启upsert/delete，只支持主键模型，用于上游是CDC数据源的时候配置。"
    starrocks_config = "StarRocks配置参数"
    starrocks_config_description = "eg: \n{'{'}\n format=\"JSON\"\n strip_outer_array=true\n{'}'}"
    save_mode_create_template = "自动建表模板"
}

S3Redshift {
    jdbc_url = "Redshift JDBC连接地址"
    jdbc_user = "Redshift JDBC用户名"
    jdbc_password = "Redshift JDBC用户密码"
    execute_sql = "Redshift Copy指令"
    execute_sql_description = "从s3导入数据到RedShift的sql语句，例如：COPY table_name FROM 's3://test${'{'}path{'}'}' IAM_ROLE 'arn:aws-cn:iam::xxx' REGION 'cn-north-1' removequotes emptyasnull blanksasnull maxerror 100 delimiter '|' ;。"
    path = "路径"
    path_description = "读取或写入数据的路径，例如: /seatunnel/path1"
    file_format_type = "文件格式"
    bucket = "S3桶地址"
    fs_s3a_endpoint = "S3a Endpoint"
    fs_s3a_endpoint_description = "例如: s3.cn-north-1.amazonaws.com.cn"
    fs_s3a_aws_credentials_provider = "S3 Credentials Provider"
    read_columns = "读取的字段列表"
    read_columns_description = "需要从文件中读取的字段列表，应该是文件中所有字段的子集"
    access_key = "Access Key"
    access_secret = "Access Secret"
    hadoop_s3_properties = "Hadoop S3 Api可选参数"
    hadoop_s3_properties_description = "使用hadoop s3 api访问s3可选的其它参数. 比如: \n{'{'}fs.s3a.buffer.dir = /data/st_test/s3a\nfs.s3a.fast.upload.buffer = disk\n{'}'}"
    delimiter = "文件中的字段分隔符"
    delimiter_description = "文件中使用的字段分隔符, 默认值是 \\001"
    parse_partition_from_path = "是否从子目录中解析分区字段"
    parse_partition_from_path_description = "是否从子目录中解析分区字段"
    date_format = "日期格式化"
    date_format_description = "日期类型的格式化，告诉同步任务该如何将字符串转换成日志，目前支持：yyyy-MM-dd， yyyy.MM.dd， yyyy/MM/dd。 默认值是 yyyy-MM-dd"
    datetime_format = "日期时间格式化"
    datetime_format_description = "Datetime类型格式，用于告诉同步任务如何将字符串转换为Datetime，支持以下格式: yyyy-MM-dd HH:mm:ss yyyy.MM.dd HH:mm:ss yyyy/MM/dd HH:mm:ss yyyyMMddHHmmss. 默认值是 yyyy-MM-dd HH:mm:ss"
    time_format = "时间类型格式化"
    time_format_description = "时间类型格式，用于告诉连接器如何将字符串转换为时间，支持以下格式: HH:mm:ss HH:mm:ss.SSS. 默认值是 HH:mm:ss"
    skip_header_row_number = "跳过文件的前几行"
    skip_header_row_number_description = "跳过前几行，仅用于text和csv文件格式类型。"
    schema = "schema"
}

SSH {
    ip = "IP地址"
    port = "端口"
    username = "用户名"
    password = "密码"
    privateKey = "目标服务器的私钥"
}

PROXY {
    url = "代理服务器IP地址"
    port = "代理服务器端口"
    username = "用户名"
    password = "密码"
}
