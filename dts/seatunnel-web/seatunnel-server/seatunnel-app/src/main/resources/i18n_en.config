#/*
 # * Licensed to the Apache Software Foundation (ASF) under one or more
 # * contributor license agreements.  See the NOTICE file distributed with
 # * this work for additional information regarding copyright ownership.
 # * The ASF licenses this file to You under the Apache License, Version 2.0
 # * (the "License"); you may not use this file except in compliance with
 # * the License.  You may obtain a copy of the License at
 # *
 # *    http://www.apache.org/licenses/LICENSE-2.0
 # *
 # * Unless required by applicable law or agreed to in writing, software
 # * distributed under the License is distributed on an "AS IS" BASIS,
 # * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 # * See the License for the specific language governing permissions and
 # * limitations under the License.
 # */
Jdbc {
    url = "JDBC Url"
    url_description = "The URL of the JDBC connection. Refer to a case: jdbc:mysql://localhost:3306/test?enabledTLSProtocols=TLSv1.2&rewriteBatchedStatements=true"
    driver = "JDBC Driver"
    driver_description = "The JDBC driver class for the database. eg: com.mysql.cj.jdbc.Driver"
    user = "User"
    password = "Password"
    query = "Query"
    query_description = "The sql to read data from source"
    connection_check_timeout_sec = "Connection Check Timeout"
    connection_check_timeout_sec_description = "Timeout for connection check"
    partition_column = "Column For Split"
    partition_column_description = "The column can be used to split the data, It is recommended to use the self-increasing primary key in the table or the primary key of number type with sufficient hash"
    partition_upper_bound = "Max value of split column"
    partition_upper_bound_description = "The max value of the split column"
    partition_lower_bound = "Min value of split column"
    partition_lower_bound_description = "The min value of the split column"
    partition_num = "Split Number"
    partition_num_description = "How many split you want to use, default value is equals to parallelism"
    fetch_size = "Fetch Size"
    fetch_size_description = "You can change the number of rows retrieved with each trip to the database cursor by changing the row fetch size value"
    parallelism = "Parallelism"
    parallelism_description = "The parallel threads are used to read data. The default is 1"
    database = "Database"
    table = "Table"
    primary_keys = "Primary Keys"
    primary_keys_description = "The primary keys of the table"
    support_upsert_by_query_primary_key_exist = "Upsert by primary key"
    support_upsert_by_query_primary_key_exist_description = "Support upsert by query primary key if exist"
    max_retries = "Max Retries"
    batch_interval_ms = "Batch Interval"
    batch_interval_ms_description = "The maximum interval between two write batches, Unit is ms, default value is 1000"
    batch_size = "Batch Size"
    batch_size_description = "The maximum rows a batch"
    is_exactly_once = "Enable Exactly Once"
    is_exactly_once_description = "Enable XA transaction to ensure accurate data writing"
    xa_data_source_class_name = "XA class name"
    xa_data_source_class_name_description = "The XA class name for the database"
    max_commit_attempts = "Max Commit Attempts"
    max_commit_attempts_description = "If transaction commit failed, retry times"
    transaction_timeout_sec = "Timeout for transaction"
    transaction_timeout_sec_description = "Timeout for transaction, Unit is second"
    auto_commit = "Auto Commit"
    auto_commit_description = "Auto commit transaction"
}

S3File {
    path = "Path"
    path_description = "The path in the s3 bucket. eg: /seatunnel/path1"
    file_format_type = "File FormatType"
    bucket = "S3 Bucket"
    fs_s3a_endpoint = "S3a Endpoint"
    fs_s3a_endpoint_description = "eg: s3.cn-north-1.amazonaws.com.cn"
    fs_s3a_aws_credentials_provider = "S3 Credentials Provider"
    read_columns = "Read Columns"
    read_columns_description = "The columns you want to read from the file"
    access_key = "Access Key"
    access_secret = "Access Secret"
    hadoop_s3_properties = "Hadoop S3 Properties"
    hadoop_s3_properties_description = "The hadoop s3 api config properties. eg: \n{'{'}fs.s3a.buffer.dir = /data/st_test/s3a\nfs.s3a.fast.upload.buffer = disk\n{'}'}"
    delimiter = "Field Delimiter"
    delimiter_description = "The field delimiter in the file, used when file format type is text, default value is \\001"
    parse_partition_from_path = "Use Subdirectories As Partitions"
    parse_partition_from_path_description = "Use subdirectories as partitions"
    date_format = "Date Format"
    date_format_description = "Date type format, used to tell connector how to convert string to date, supported as the following formats. Default value is yyyy-MM-dd"
    datetime_format = "Datetime Format"
    datetime_format_description = "Datetime type format, used to tell connector how to convert string to datetime, supported as the following formats: yyyy-MM-dd HH:mm:ss yyyy.MM.dd HH:mm:ss yyyy/MM/dd HH:mm:ss yyyyMMddHHmmss. Default value is yyyy-MM-dd HH:mm:ss"
    time_format = "Time Format"
    time_format_description = "Time type format, used to tell connector how to convert string to time, supported as the following formats: HH:mm:ss HH:mm:ss.SSS. Default value is HH:mm:ss"
    skip_header_row_number = "Skip Header Row Number"
    skip_header_row_number_description = "Skip the first few lines, only used for text and csv file format type."
    schema = "schema"
}

MySQL-CDC {
    username = "Username"
    password = "Password"
    base_url = "Base Url"
    server_time_zone = "Server TimeZone"
    format = "Data Output Format"
    DEFAULT = "Default"
    COMPATIBLE_DEBEZIUM_JSON = "Compatible Debezium Json"
    snapshot_split_size = "Snapshot Split Size"
    snapshot_split_size_description = "In the historical data reading stage, the number of data rows read by each split. Each table will be split into multiple split for parallel reading"
    snapshot_fetch_size = "Snapshot Fetch Size"
    snapshot_fetch_size_description = "JDBC fetch size when reading table snapshots"
    incremental_parallelism = "Incremental Stage Parallelism"
    incremental_parallelism_description = "The number of parallels in the incremental read state. Only supported to be configured as 1 now"
    startup_mode = "Startup mode"
    INITIAL = "Read historical data and incremental data"
    EARLIEST = "Read incremental data from the smallest binlog location"
    LATEST = "Read incremental data from the latest binlog location"
    SPECIFIC = "Read incremental data from the binlog at the specified location"
    TIMESTAMP = "Read from the binlog at a certain point in time"
    startup_specific_offset_file = "Begin Read Log File And Offset"
    startup_specific_offset_file_description = "Configure the file and location to start reading"
    startup_specific_offset_pos = "Begin Read Log File Offset"
    startup_specific_offset_pos_description = "Configure the location to start reading"
    stop_mode = "Stop Mode"
    NEVER = "Won't Be Finished"
    debezium = "Debezium Config"
    debezium_description = "Pass the properties of Debezium to the Debezium Embedded Engine, which is used to capture data changes from the MySQL server. You can get more information from https://debezium.io/documentation/reference/1.6/connectors/mysql.html#mysql-connector-properties. eg: \n{'{'}\n snapshot.mode = \"never\"\n decimal.handling.mode = \"double\"\n{'}'}"
    server_id = "MySQL Server Id"
    connect_timeout_ms = "Connect Timeout"
    connect_timeout_ms_description = "The maximum time the connector should wait after trying to connect to the database server before timeout"
    connect_max_retries = "Connect Max Retries"
    connect_max_retries_description = "The maximum number of retries that the connector can retry to establish a database server connection"
    connection_pool_size = "Connection Pool Size"
    connection_pool_size_description = "Database connection pool size"
    parallelism = "Parallelism"
    parallelism_description = "The parallelism when reading historical data. It affects the speed of reading historical data."
}

SqlServer-CDC {
    username = "Username"
    password = "Password"
    base_url = "Base Url"
    server_time_zone = "Server TimeZone"
    format = "Data Output Format"
    DEFAULT = "Default"
    COMPATIBLE_DEBEZIUM_JSON = "Compatible Debezium Json"
    snapshot_split_size = "Snapshot Split Size"
    snapshot_split_size_description = "In the historical data reading stage, the number of data rows read by each split. Each table will be split into multiple split for parallel reading"
    snapshot_fetch_size = "Snapshot Fetch Size"
    snapshot_fetch_size_description = "JDBC fetch size when reading table snapshots"
    incremental_parallelism = "Incremental Stage Parallelism"
    incremental_parallelism_description = "The number of parallels in the incremental read state. Only supported to be configured as 1 now"
    startup_mode = "Startup mode"
    INITIAL = "Read historical data and incremental data"
    EARLIEST = "Read incremental data from the smallest binlog location"
    LATEST = "Read incremental data from the latest binlog location"
    SPECIFIC = "Read incremental data from the binlog at the specified location"
    TIMESTAMP = "Read from the binlog at a certain point in time"
    startup_specific_offset_file = "Begin Read Log File And Offset"
    startup_specific_offset_file_description = "Configure the file and location to start reading"
    startup_specific_offset_pos = "Begin Read Log File Offset"
    startup_specific_offset_pos_description = "Configure the location to start reading"
    stop_mode = "Stop Mode"
    NEVER = "Won't Be Finished"
    debezium = "Debezium Config"
    debezium_description = "Pass the properties of Debezium to the Debezium Embedded Engine, which is used to capture data changes from the MySQL server. You can get more information from https://debezium.io/documentation/reference/1.6/connectors/mysql.html#mysql-connector-properties. eg: \n{'{'}\n snapshot.mode = \"never\"\n decimal.handling.mode = \"double\"\n{'}'}"
    server_id = "MySQL Server Id"
    connect_timeout_ms = "Connect Timeout"
    connect_timeout_ms_description = "The maximum time the connector should wait after trying to connect to the database server before timeout"
    connect_max_retries = "Connect Max Retries"
    connect_max_retries_description = "The maximum number of retries that the connector can retry to establish a database server connection"
    connection_pool_size = "Connection Pool Size"
    connection_pool_size_description = "Database connection pool size"
    parallelism = "Parallelism"
    parallelism_description = "The parallelism when reading historical data. It affects the speed of reading historical data."
}

Elasticsearch {
    hosts = "Hosts"
    hosts_description = "The HTTP port information in ElasticSearch is of type List, such as [\"127.0.0.1:9200\", \"127.0.0.2:9200\"]"
    username = "Username"
    password = "Password"
    tls_verify_certificate = "Enable TLS Verify Certificate"
    tls_verify_certificate_description = "Enable certificate validation for HTTPS endpoints"
    tls_verify_hostname = "Enable TLS Verify Hostname"
    tls_verify_hostname_description = "Enable hostname validation for HTTPS endpoints"
    tls_keystore_path = "TLS Keystore Path"
    tls_keystore_path_description = "The path to the PEM or JKS keystore. The operating system user running WhaleTunnel must be able to read the file"
    tls_keystore_password = "TLS Keystore Password"
    tls_keystore_password_description = "Specify the key password for the KeyStore"
    tls_truststore_path = "TLS Truststore Path"
    tls_truststore_path_description = "The path of the PEM or JKS truststore. The operating system user running WhaleTunnel must be able to read the file"
    tls_truststore_password = "Specify the key password for the truststore"
    scroll_time = "Scroll Time"
    scroll_time_description = "Elasticsearch will keep the search context active for scrolling requests"
    scroll_size = "Scroll Size"
    scroll_size_description = "Maximum number of hits returned per Elasticsearch scroll request"
    parallelism = "Parallelism"
    parallelism_description = "Parallelism. The greater the parallelism, the faster the read speed, and the more machine resources consumed"
    index_type = "Index Type"
    index_type_description = "Elasticsearch index type, it is recommended not to specify in Elasticsearch 6 and above"
    primary_keys = "Primary Keys"
    primary_keys_description = "Used to generate documents _id Primary key field, which is a required option for CDC mode (eg: Source is MySQL-CDC)."
    key_delimiter = "Key Delimiter"
    key_delimiter_description = "The separator of the compound key (the default is \"_\"), for example, \"$\" will generate a document _id \"KEY1$KEY2$KEY3\""
    max_retry_count = "Max Retry Count"
    max_batch_size = "Max Batch Size"
    max_batch_size_description = "Maximum data size of bulk write requests"
}

Kafka {
    bootstrap_servers = "bootstrap.servers"
    bootstrap_servers_description = "bootstrap.servers. eg: host1:9092, host2:9092"
    kafka_config = "Kafka Config"
    kafka_config_description = "Used to enter some additional parameters, using HOCON format, such as \n{'{'}\n max.pool.recodes=500\n{'}'}"
    topic = "topic"
    pattern = "pattern"
    start_mode = "Start Mode"
    EARLIEST = "From The Earliest Offset"
    GROUP_OFFSETS = "From The Group Offset"
    LATEST = "From The Latest Offset"
    TIMESTAMP = "From The Specified Timestamp"
    SPECIFIC_OFFSETS = "From The Specified Offset"
    consumer_group = "Consumer Group"
    consumer_group_description = "Consumer Group，default value is SeaTunnel-Consumer-Group"
    commit_on_checkpoint = "Commit On Checkpoint"
    commit_on_checkpoint_description = "Whether to submit Offset when Checkpoint is triggered"
    format = "Data Format"
    partition_discovery_interval_millis = "partition-discovery.interval-millis"
    partition_discovery_interval_millis_description = "The time interval for querying the partition. If the setting is greater than 0, the partition can be dynamically discovered at runtime"
    parallelism = "Parallelism"
}

StarRocks {
    nodeUrls = "Node Http Urls"
    nodeUrls_description = "HTTP port information in StarRocks, type is List, eg: [\"127.0.0.1:8030\"]"
    username = "Username"
    password = "Password"
    base_url = "JDBC Url"
    base_url_description = "eg: jdbc:mysql://127.0.0.1:9030/test"
    labelPrefix = "Label Prefix"
    labelPrefix_description = "Label used to specify the import job. If you do not specify a label, StarRocks will automatically generate a label for the import job."
    batch_max_rows = "Max Rows Each Batch"
    batch_max_bytes = "Max bytes Each Batch"
    batch_interval_ms = "Batch Interval(ms)"
    max_retries = "Max Retries"
    max_retry_backoff_ms = "Max Retry Backoff(ms)"
    retry_backoff_multiplier_ms = "Retry Backoff Multiplier(ms)"
    enable_upsert_delete = "Enable Upsert/Delete"
    enable_upsert_delete_description = "Whether to enable upsert/delete only supports the primary key model, which is used to configure when the upstream is the CDC data source."
    starrocks_config = "StarRocks Config"
    starrocks_config_description = "eg: \n{'{'}\n format=\"JSON\"\n strip_outer_array=true\n{'}'}"
    save_mode_create_template = "Create Table Template"
}


S3Redshift {
    jdbc_url = "Redshift JDBC Url"
    jdbc_user = "Redshift JDBC Username"
    jdbc_password = "Redshift JDBC Password"
    execute_sql = "Redshift Copy Command"
    execute_sql_description = "eg: COPY table_name FROM 's3://test${'{'}path{'}'}' IAM_ROLE 'arn:aws-cn:iam::xxx' REGION 'cn-north-1' removequotes emptyasnull blanksasnull maxerror 100 delimiter '|' ;。"
    path = "Path"
    path_description = "The path in the s3 bucket. eg: /seatunnel/path1"
    file_format_type = "File FormatType"
    bucket = "S3 Bucket"
    fs_s3a_endpoint = "S3a Endpoint"
    fs_s3a_endpoint_description = "eg: s3.cn-north-1.amazonaws.com.cn"
    fs_s3a_aws_credentials_provider = "S3 Credentials Provider"
    read_columns = "Read Columns"
    read_columns_description = "The columns you want to read from the file"
    access_key = "Access Key"
    access_secret = "Access Secret"
    hadoop_s3_properties = "Hadoop S3 Properties"
    hadoop_s3_properties_description = "The hadoop s3 api config properties. eg: \n{'{'}fs.s3a.buffer.dir = /data/st_test/s3a\nfs.s3a.fast.upload.buffer = disk\n{'}'}"
    delimiter = "Field Delimiter"
    delimiter_description = "The field delimiter in the file, used when file format type is text, default value is \\001"
    parse_partition_from_path = "Use Subdirectories As Partitions"
    parse_partition_from_path_description = "Use subdirectories as partitions"
    date_format = "Date Format"
    date_format_description = "Date type format, used to tell connector how to convert string to date, supported as the following formats. Default value is yyyy-MM-dd"
    datetime_format = "Datetime Format"
    datetime_format_description = "Datetime type format, used to tell connector how to convert string to datetime, supported as the following formats: yyyy-MM-dd HH:mm:ss yyyy.MM.dd HH:mm:ss yyyy/MM/dd HH:mm:ss yyyyMMddHHmmss. Default value is yyyy-MM-dd HH:mm:ss"
    time_format = "Time Format"
    time_format_description = "Time type format, used to tell connector how to convert string to time, supported as the following formats: HH:mm:ss HH:mm:ss.SSS. Default value is HH:mm:ss"
    skip_header_row_number = "Skip Header Row Number"
    skip_header_row_number_description = "Skip the first few lines, only used for text and csv file format type."
    schema = "schema"
}

SSH {
    ip = "IP"
    port = "Port"
    username = "Username"
    password = "Password"
    privateKey = "Private key of the target server"
}

PROXY {
    url = "Proxy server url"
    port = "port"
    username = "username"
    password = "password"
}
